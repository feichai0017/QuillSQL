<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>QuillSQL Internals</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">QuillSQL Internals</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>Welcome to the technical documentation for QuillSQL.</p>
<p>This book provides a deep dive into the internal architecture and implementation details of the database. It is intended for developers, contributors, and anyone interested in understanding how a relational database is built from the ground up.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quillsql-architecture"><a class="header" href="#quillsql-architecture">QuillSQL Architecture</a></h1>
<p>This document explains how a SQL request flows through QuillSQL, how transactional MVCC and background services plug in, and how the main modules collaborate. All diagrams use Mermaid so you can paste them into any compatible renderer for a richer view.</p>
<hr />
<h2 id="1-end-to-end-request-pipeline"><a class="header" href="#1-end-to-end-request-pipeline">1. End-to-End Request Pipeline</a></h2>
<pre><code class="language-mermaid">flowchart TD
    Client["CLI / HTTP client"] --&gt; API["bin/client / bin/server"]
    API --&gt; Parser["sql::parser"]
    Parser --&gt; LPlanner["plan::LogicalPlanner"]
    LPlanner --&gt; Optimizer["optimizer::LogicalOptimizer"]
    Optimizer --&gt; PhysPlanner["plan::PhysicalPlanner"]
    PhysPlanner --&gt; Exec["execution::ExecutionEngine (Volcano)"]

    subgraph Txn["Transaction Layer"]
        Session["session::SessionContext"]
        TM["transaction::TransactionManager"]
        LM["transaction::LockManager"]
        Session --&gt; TM --&gt; LM
    end

    Exec --&gt;|Tuple meta &amp; locks| Txn
    Exec --&gt; Storage

    subgraph Storage["Storage &amp; I/O"]
        TableHeap["storage::table_heap::TableHeap"]
        Index["storage::index::B+Tree"]
        Buffer["buffer::BufferManager"]
        Scheduler["storage::disk_scheduler (io_uring)"]
        WAL["recovery::WalManager"]
        TableHeap &lt;--&gt; Buffer
        Index &lt;--&gt; Buffer
        Buffer &lt;--&gt; Scheduler
        WAL --&gt; Scheduler
    end

    Txn --&gt;|WAL payloads| WAL

    Background["background::workers\n(checkpoint, bg writer, MVCC vacuum)"] --&gt; Buffer
    Background --&gt; WAL
    Background --&gt; TM
</code></pre>
<p><strong>High-level flow</strong></p>
<ol>
<li>SQL text is parsed into an AST, planned into a <code>LogicalPlan</code>, optimized by a handful of safe rewrite rules, then lowered into a physical operator tree.</li>
<li><code>SessionContext</code> either reuses or creates a transaction and injects isolation/access modes.</li>
<li>Each physical operator is driven by the Volcano pull model (<code>init</code>/<code>next</code>). On entry it obtains a <code>TxnRuntime</code> which supplies a command id plus an MVCC snapshot consistent with the transaction’s isolation level.</li>
<li>Operators consult the snapshot for tuple visibility, acquire table/row locks through <code>TxnRuntime</code>, and issue heap/index operations.</li>
<li>DML operators register undo records and append WAL entries via the transaction manager. When the user commits, the manager emits <code>Commit</code> records, waits for durability as configured, and releases locks.</li>
</ol>
<hr />
<h2 id="2-transaction--mvcc-mechanics"><a class="header" href="#2-transaction--mvcc-mechanics">2. Transaction &amp; MVCC Mechanics</a></h2>
<pre><code class="language-mermaid">sequenceDiagram
    participant Session
    participant TM as TransactionManager
    participant Runtime as TxnRuntime
    participant Exec as Operator
    participant Heap as TableHeap

    Session-&gt;&gt;TM: begin(isolation, access_mode)
    TM--&gt;&gt;Session: Transaction handle
    Exec-&gt;&gt;Runtime: TxnRuntime::new(&amp;TM, &amp;mut txn)
    Runtime--&gt;&gt;Exec: {snapshot, command_id}
    loop per tuple
        Exec-&gt;&gt;Heap: next()
        Heap--&gt;&gt;Exec: (rid, meta, tuple)
        Exec-&gt;&gt;Runtime: is_visible(meta)?
        Runtime--&gt;&gt;Exec: yes/no (uses cached snapshot)
        alt Visible
            Exec-&gt;&gt;Runtime: lock_row(...)
            Exec-&gt;&gt;TM: record undo + WAL
        end
    end
    Session-&gt;&gt;TM: commit(txn)
    TM-&gt;&gt;WAL: Transaction(Commit)
    TM-&gt;&gt;TM: wait_for_durable / flush_until
    TM--&gt;&gt;Session: release locks, clear snapshot
</code></pre>
<ul>
<li>
<p><strong>Snapshots</strong></p>
<ul>
<li><em>Read Committed / Read Uncommitted:</em> Every command refreshes its snapshot and clears any cached value on the transaction handle.</li>
<li><em>Repeatable Read / Serializable:</em> The first command captures a snapshot (<code>xmin</code>, <code>xmax</code>, <code>active_txns</code>) and stores it inside <code>Transaction</code>. Subsequent commands reuse it, ensuring consistent reads.</li>
<li>Background MVCC vacuum consults <code>TransactionManager::oldest_active_txn()</code> to compute <code>safe_xmin</code> and prunes tuple versions whose inserting/deleting transactions precede that boundary.</li>
</ul>
</li>
<li>
<p><strong>Locking</strong><br />
Multi-granularity 2PL (IS/IX/S/SIX/X) enforced by <code>LockManager</code>. Repeatable Read releases shared locks at the end of each command (after verifying visibility). Serializable keeps shared locks through commit. Deadlocks are detected via a wait-for graph; a victim simply fails its lock request.</p>
</li>
<li>
<p><strong>Undo &amp; WAL</strong><br />
<code>Transaction</code> maintains logical undo entries. On abort, the manager emits CLR records and performs the inverse heap operations. Commit waits depend on <code>synchronous_commit</code>. Buffer frames retain their <code>page_lsn</code> so WAL-before-data holds.</p>
</li>
<li>
<p><strong>Executor safeguards</strong><br />
<code>PhysicalUpdate</code> now skips tuple versions created by the same <code>(txn_id, command_id)</code> during the current command. This prevents re-processing the freshly inserted MVCC version and thereby avoids infinite loops.</p>
</li>
</ul>
<hr />
<h2 id="3-storage--buffering"><a class="header" href="#3-storage--buffering">3. Storage &amp; Buffering</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Component</th><th>Highlights</th></tr></thead><tbody>
<tr><td><code>TableHeap</code></td><td>Tuple metadata (<code>TupleMeta</code>) stores inserting/deleting txn ids, command ids, and forward/back pointers for version chains. Helpers like <code>mvcc_update</code> stitch new versions while marking old ones deleted.</td></tr>
<tr><td><code>B+Tree</code></td><td>B-link tree implementation with separate codecs for header/internal/leaf pages. Index maintenance occurs in DML operators after the heap change succeeds.</td></tr>
<tr><td><code>BufferManager</code></td><td>Combines a page table, LRU-K replacer (with TinyLFU admission option), and per-frame guards. Dirty pages record their first-dirty LSN, feeding checkpoints. The background writer periodically flushes dirty frames and drives lazy index cleanup.</td></tr>
<tr><td><code>DiskScheduler</code></td><td>Uses io_uring worker threads. Foreground tasks push <code>ReadPage</code>, <code>WritePage</code>, <code>WriteWal</code>, and <code>FsyncWal</code> commands through lock-free queues and receive completion on dedicated channels.</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="4-write-ahead-logging--recovery"><a class="header" href="#4-write-ahead-logging--recovery">4. Write-Ahead Logging &amp; Recovery</a></h2>
<ul>
<li><code>WalManager</code> manages log sequence numbers, buffers frames, writes physical (<code>PageWrite</code>, <code>PageDelta</code>) and logical (<code>HeapInsert/Update/Delete</code>) records, and coordinates flushes. First-page-writes guard against torn pages.</li>
<li><code>background::spawn_checkpoint_worker</code> emits <code>Checkpoint</code> records capturing the dirty page table and active transactions so recovery can cut replay short.</li>
<li><code>RecoveryManager</code> executes ARIES-style <strong>analysis → redo → undo</strong> on restart, leveraging CLRs to keep undo idempotent.</li>
<li>WAL and data I/O both use the <code>DiskScheduler</code>, keeping durability guarantees in one place.</li>
</ul>
<hr />
<h2 id="5-background-services"><a class="header" href="#5-background-services">5. Background Services</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Worker</th><th>Description</th><th>Trigger</th></tr></thead><tbody>
<tr><td>WAL writer</td><td>Coalesces buffered WAL into durable segments</td><td><code>WalManager::start_background_flush</code></td></tr>
<tr><td>Checkpoint</td><td>Flushes LSNs, records ATT + DPT snapshots, resets FPW epoch</td><td>Configurable interval (<code>WalOptions::checkpoint_interval_ms</code>)</td></tr>
<tr><td>Buffer writer</td><td>Flushes dirty frames, runs index lazy cleanup based on pending garbage counters</td><td><code>IndexVacuumConfig</code></td></tr>
<tr><td>MVCC vacuum</td><td>Iterates table heaps, removing committed-deleted or aborted-inserted tuples older than <code>safe_xmin</code></td><td><code>MvccVacuumConfig</code> (interval + batch limit)</td></tr>
</tbody></table>
</div>
<p>All workers are registered with <code>background::BackgroundWorkers</code>, which stops and joins them on database shutdown.</p>
<hr />
<h2 id="6-example-timeline-repeatable-read-update"><a class="header" href="#6-example-timeline-repeatable-read-update">6. Example Timeline: Repeatable Read UPDATE</a></h2>
<pre><code>T1 (RR)                               T2 (RC)
-----------                           -----------
BEGIN;                                BEGIN;
SELECT ... (snapshot S0)              UPDATE row -&gt; new version V1
                                      COMMIT (WAL + flush)
SELECT again -&gt; sees original value
COMMIT
-- background vacuum later reclaims deleted version when safe_xmin &gt; delete_txn
</code></pre>
<ul>
<li>T1’s <code>TxnRuntime</code> caches snapshot S0 on its first command and reuses it, so the second <code>SELECT</code> filters out V1 even though T2 committed.</li>
<li>Row-level shared locks acquired during the read are released at the end of the command, while the MVCC snapshot keeps the view consistent.</li>
<li>When T1 commits, locks are dropped, snapshot cache is cleared, and WAL commit record becomes durable. Vacuum eventually removes T1’s deleted predecessor when all transactions with <code>txn_id &lt; safe_xmin</code> have finished.</li>
</ul>
<hr />
<h2 id="7-observability--configuration"><a class="header" href="#7-observability--configuration">7. Observability &amp; Configuration</a></h2>
<ul>
<li>Enable tracing via <code>RUST_LOG=trace</code> to inspect lock grant/queue events and MVCC vacuum activity.</li>
<li>Key knobs exposed through CLI/environment: WAL segment size, background intervals, default isolation level, MVCC vacuum batch size.</li>
<li><code>background::BackgroundWorkers::snapshot()</code> reports active worker metadata; you can surface it for debugging endpoints.</li>
</ul>
<hr />
<h2 id="8-roadmap"><a class="header" href="#8-roadmap">8. Roadmap</a></h2>
<ul>
<li>Predicate locking / SSI to upgrade Serializable beyond strict 2PL.</li>
<li>Cost-based optimization with catalog statistics.</li>
<li>Smarter vacuum pacing tied to storage pressure and tuple churn.</li>
<li>Parallel execution and adaptive readahead hints based on operator feedback.</li>
</ul>
<p>Even with these future items, the current layering mirrors production databases (e.g., PostgreSQL): MVCC + 2PL, ARIES-style WAL, asynchronous maintenance, and a modular Volcano executor—all while keeping the codebase approachable for teaching and experimentation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="buffer-manager--architecture-and-streaming-scan"><a class="header" href="#buffer-manager--architecture-and-streaming-scan">Buffer Manager — Architecture and Streaming Scan</a></h1>
<h2 id="1-architecture-overview"><a class="header" href="#1-architecture-overview">1. Architecture Overview</a></h2>
<p>The Buffer Manager is responsible for managing pages in memory, acting as a cache between the disk and the execution engine. It fetches pages from disk into memory frames, allows threads to "pin" them for safe access, and writes dirty pages back to disk while coordinating with WAL.</p>
<h3 id="11-core-components"><a class="header" href="#11-core-components">1.1 Core Components</a></h3>
<pre><code>+-------------------------------------------------------------------+
|                           Buffer Manager                           |
|                                                                   |
|  +-----------------+   +------------------+   +------------------+  |
|  |   Page Table    |   |    Replacer      |   |    Free List     |  |
|  | (DashMap&lt;P,F&gt;)  |&lt;-&gt;|    (LRU-K)       |&lt;-&gt;|  (VecDeque&lt;F&gt;)   |  |
|  +-----------------+   +------------------+   +------------------+  |
|        ^   |                        ^                               |
|        |   |                        |                               |
|        |   v                        |                               |
|  +----------------------------------------------------------------+ |
|  |                           Frame Pool                           | |
|  |              (Vec&lt;Arc&lt;RwLock&lt;Page&gt;&gt;&gt; + FrameMeta)               | |
|  |  ┌────────────┬────────────┬────────────┬────────────┐         | |
|  |  | Frame 0     | Frame 1    | Frame 2    | Frame 3    |  ...    | |
|  |  | page_id=34  | page_id=8  | page_id=0  | page_id=55 |         | |
|  |  | LeafPage    | Internal   | FREE       | TablePage  |         | |
|  |  | pin=2 dirty | pin=0      | pin=0      | pin=1 dirty|         | |
|  |  └────────────┴────────────┴────────────┴────────────┘         | |
|  +----------------------------------------------------------------+ |
|        ^   |                        ^   |                           |
|        |   | (pins/unpins)          |   | (page bytes)              |
|        |   v                        |   v                           |
|  +--------------------+              +---------------------+        |
|  |   Client Thread    |              |    Disk Scheduler   |        |
|  | (e.g., Executor)   |&lt;------------&gt;|  (I/O Background    |        |
|  | - fetch_page_*()   |              |    Thread)          |        |
|  | - new_page()       |              | - read/write/alloc  |        |
|  | - prefetch_page()  |              +---------------------+        |
|  | - unpin_page()     |                       ^                    |
|  +--------------------+                       |                    |
|        ^                                       |                    |
|        |                                       v                    |
|   +-----------+                       +--------------------+        |
|   | BG Writer |----------------------&gt;| flush_page batches |        |
|   +-----------+                       +--------------------+        |
|                                                                   | |
|                                                       +----------------+
|                                                       |    Disk File   |
|                                                       +----------------+
+-------------------------------------------------------------------+
</code></pre>
<ul>
<li>
<p><strong>Frame Pool (<code>pool</code>)</strong>: A <code>Vec&lt;Arc&lt;RwLock&lt;Page&gt;&gt;&gt;</code> representing the main memory area managed by the Buffer Manager. Each frame is protected by its own <code>RwLock</code> for fine-grained concurrent access, and <code>FrameMeta</code> captures <code>page_id</code>, <code>pin_count</code>, <code>is_dirty</code>, and <code>lsn</code> for eviction and durability decisions.</p>
</li>
<li>
<p><strong>Page Table (<code>page_table</code>)</strong>: A <code>DashMap&lt;PageId, FrameId&gt;</code> for efficiently mapping logical page IDs to their physical frame locations. <code>DashMap</code> provides high-performance concurrent lookups, insertions, and removals.</p>
</li>
<li>
<p><strong>Replacer (<code>replacer</code>)</strong>: An <code>LRU-K</code> replacer that implements the page replacement policy. When the free list is empty, it selects a victim frame to evict. The replacer is internally sharded to reduce lock contention.</p>
</li>
<li>
<p><strong>Free List (<code>free_list</code>)</strong>: A <code>VecDeque&lt;FrameId&gt;</code> that tracks available frames, allowing for quick allocation without needing to consult the replacer.</p>
</li>
<li>
<p><strong>Disk Scheduler (<code>disk_scheduler</code>)</strong>: Asynchronous I/O backend. The Buffer Manager offloads reads, writes, allocate/deallocate, and WAL requests to the scheduler via channels so client threads do not block on syscalls.</p>
</li>
<li>
<p><strong>Dirty Page Table (<code>dirty_page_table</code>)</strong>: A <code>DashMap&lt;PageId, Lsn&gt;</code> that tracks the first log sequence number (<code>rec_lsn</code>) that dirtied each page. This snapshot feeds checkpoints and recovery decisions. The table always records the earliest <code>lsn</code> emitted by the current write guard so the background writer can flush in WAL order.</p>
</li>
<li>
<p><strong>Background Writer</strong>: Optional thread that periodically calls <code>flush_page</code> on batches of dirty frames, smoothing out latency for foreground work.</p>
</li>
</ul>
<h3 id="12-frame--page-layout"><a class="header" href="#12-frame--page-layout">1.2 Frame &amp; Page Layout</a></h3>
<p>The Buffer Pool keeps hot pages in a compact arena. Each frame holds one page-sized slice plus metadata:</p>
<pre><code>FrameMeta {
    page_id: PageId,       // INVALID_PAGE_ID if the slot is free
    pin_count: u32,        // &gt;0 blocks eviction
    is_dirty: bool,        // true means bytes differ from disk
    lsn: Lsn,              // latest WAL record applied to this frame
}
</code></pre>
<p>Example slice of the arena:</p>
<pre><code>Frame 0 ──┬─ Page 34 (Leaf | pin=2 | dirty | lsn=148)
          │   • entries: [10→RID(1,3), 24→RID(2,8)]
Frame 1 ──┼─ Page 8  (Internal | pin=0 | clean | lsn=120)
          │   • children: [4, 9, 17]
Frame 2 ──┼─ FREE (INVALID_PAGE_ID)
Frame 3 ──┴─ Table Page 55 (Heap | pin=1 | dirty | lsn=151)
</code></pre>
<p>Frames with <code>pin_count == 0</code> and <code>is_dirty == false</code> are immediately evictable. Dirty frames enter the <code>dirty_pages</code> set and must flush after <code>ensure_wal_durable(lsn)</code> succeeds.</p>
<h3 id="13-pin-protocol-wal-and-page-guards"><a class="header" href="#13-pin-protocol-wal-and-page-guards">1.3 Pin Protocol, WAL, and Page Guards</a></h3>
<p>To ensure safe memory access, the BPM uses a pin/unpin protocol. A page is "pinned" when a thread is actively using it, preventing it from being evicted. The pin count on a page tracks how many threads are currently using it.</p>
<p>This protocol is enforced through RAII guards:</p>
<ul>
<li><code>ReadPageGuard</code>: Provides immutable access to a page's data.</li>
<li><code>WritePageGuard</code>: Provides mutable access and records the first dirty LSN. On drop it passes a <code>rec_lsn</code> hint into the Buffer Manager so WAL ordering stays correct.</li>
</ul>
<p>When a guard is acquired (via <code>fetch_page_read</code>/<code>fetch_page_write</code>), the page's pin count is incremented, and it's marked as non-evictable. When the guard goes out of scope, its <code>Drop</code> implementation automatically decrements the pin count. If the count reaches zero, the page is marked as evictable again. This design makes memory management safe and largely automatic for the caller.</p>
<p>The Buffer Manager is WAL-aware: before flushing or evicting a dirty page, it invokes <code>ensure_wal_durable(lsn)</code> to guarantee the corresponding log records are persisted. Dirty pages are also tracked in <code>dirty_pages</code>/<code>dirty_page_table</code>, enabling checkpoints to log accurate metadata. <code>WritePageGuard::drop</code> passes the first dirty <code>lsn</code> it observed so <code>dirty_page_table</code> always points to the earliest log record that must be durable.</p>
<h3 id="14-page-lifecycle--state-transitions"><a class="header" href="#14-page-lifecycle--state-transitions">1.4 Page Lifecycle &amp; State Transitions</a></h3>
<ol>
<li><strong>Lookup</strong>: <code>page_table</code> maps a logical <code>PageId</code> to a <code>FrameId</code>. Cache hits skip disk I/O and simply increment <code>pin_count</code>.</li>
<li><strong>Miss Handling</strong>: If <code>page_id</code> is absent, <code>ensure_frame</code> either pops a free frame or consults the LRU-K replacer for a victim. Dirty victims flush after WAL durability is confirmed.</li>
<li><strong>Pin Acquisition</strong>: <code>fetch_page_read</code>/<code>fetch_page_write</code> increments <code>pin_count</code> before handing out a guard. The guard owns the frame lock (<code>RwLock</code>), enforcing isolation.</li>
<li><strong>Mutation</strong>: Writers call <code>set_lsn</code> and <code>mark_dirty</code>. The first <code>lsn</code> observed becomes the page's <code>rec_lsn</code>, enabling ARIES-style recovery.</li>
<li><strong>Unpin</strong>: Guard <code>Drop</code> calls <code>complete_unpin</code>. When <code>pin_count</code> falls to zero the frame becomes evictable. Dirty frames remain tracked until flushed.</li>
<li><strong>Flush/Evict</strong>: <code>flush_page</code> writes bytes back to disk, clears <code>is_dirty</code>, and removes the entry from dirty tracking. Eviction finally resets the frame and returns it to the free list.</li>
</ol>
<p>All state changes are serialized through per-frame locks, so metadata updates stay consistent even under heavy concurrency.</p>
<h2 id="2-buffer-pool-internals"><a class="header" href="#2-buffer-pool-internals">2. Buffer Pool Internals</a></h2>
<h3 id="21-memory-layout--data-structures"><a class="header" href="#21-memory-layout--data-structures">2.1 Memory Layout &amp; Data Structures</a></h3>
<ul>
<li><strong>Arena (<code>arena</code>)</strong>: A contiguous <code>Box&lt;[UnsafeCell&lt;u8&gt;]&gt;</code> sized at <code>capacity * PAGE_SIZE</code>. Each frame's bytes occupy a stable slot, so pointers remain valid across fetches.</li>
<li><strong>Frame Locks (<code>locks</code>)</strong>: A <code>Vec&lt;RwLock&lt;()&gt;&gt;</code> giving each frame its own synchronization primitive. Guards in <code>page.rs</code> transmute these locks into <code>'static</code> scope for RAII.</li>
<li><strong>Frame Metadata (<code>meta</code>)</strong>: A <code>Vec&lt;Mutex&lt;FrameMeta&gt;&gt;</code> storing the eviction-critical fields described above. <code>Mutex</code> keeps metadata updates isolated from page bytes.</li>
<li><strong>Page Table (<code>page_table</code>)</strong>: <code>DashMap&lt;PageId, FrameId&gt;</code> enabling lock-free lookups for hits. Removal helpers (<code>remove_mapping_if</code>) guard against stale mappings when frames are reassigned.</li>
<li><strong>Free List (<code>free_list</code>)</strong>: <code>Mutex&lt;VecDeque&lt;FrameId&gt;&gt;</code> tracking unused frames. Frames return here after eviction and reset.</li>
<li><strong>Disk Scheduler (<code>disk_scheduler</code>)</strong>: Shared async backend for reads, writes, allocations, and deallocations. BufferPool methods schedule work and wait on the returned receiver.</li>
</ul>
<h3 id="22-frame-allocation-flow"><a class="header" href="#22-frame-allocation-flow">2.2 Frame Allocation Flow</a></h3>
<ol>
<li><strong><code>pop_free_frame</code></strong>: Preferred path—O(1) pop from the free list yields an unused frame without touching the replacer.</li>
<li><strong><code>evict_victim_frame</code></strong>: When the free list is empty, BufferManager asks the LRUK replacer for a victim. BufferPool cooperates by exposing metadata and providing <code>reset_frame</code> to zero the data region.</li>
<li><strong>Metadata Reset</strong>: Eviction clears <code>FrameMeta</code> and marks <code>page_id</code> as <code>INVALID_PAGE_ID</code>, after which the frame re-enters the free list.</li>
<li><strong>Admission Checks</strong>: TinyLFU (if enabled) can reject low-frequency pages before a victim is chosen, reducing churn.</li>
</ol>
<h3 id="23-disk-io-primitives"><a class="header" href="#23-disk-io-primitives">2.3 Disk I/O Primitives</a></h3>
<ul>
<li><strong><code>load_page_into_frame</code></strong>: Schedules a disk read, copies bytes into the target frame, zero-fills any short read, and resets metadata. The copy uses the unsafe slice helpers but remains bounded by <code>PAGE_SIZE</code>.</li>
<li><strong><code>write_page_to_disk</code></strong>: Copies the in-memory page into a <code>Bytes</code> buffer and issues an async write via the scheduler. Success clears dirty flags upstream.</li>
<li><strong><code>allocate_page_id</code> / <code>schedule_deallocate</code></strong>: Abstract page-ID management so higher layers never touch the disk manager directly.</li>
</ul>
<h3 id="24-invariants--safety-notes"><a class="header" href="#24-invariants--safety-notes">2.4 Invariants &amp; Safety Notes</a></h3>
<ul>
<li>Frame bytes must only be accessed while holding the corresponding <code>RwLock</code>; guards enforce this.</li>
<li><code>FrameMeta.pin_count &gt; 0</code> implies the frame is marked non-evictable by the replacer.</li>
<li>Disk scheduler channels are assumed reliable; every caller maps disconnection into <code>QuillSQLError::Internal</code>.</li>
<li><code>reset_frame</code> always zeroes the buffer to prevent dirty data from leaking into newly allocated pages.</li>
</ul>
<p>These invariants keep BufferPool deterministic even under concurrent fetches and evictions.</p>
<h2 id="3-concurrency--safety"><a class="header" href="#3-concurrency--safety">3. Concurrency &amp; Safety</a></h2>
<ul>
<li>
<p><strong>Per-Frame Locking</strong>: Using an <code>RwLock</code> for each <code>Page</code> allows multiple threads to read the same page concurrently, or one thread to write to it, without blocking access to other pages.</p>
</li>
<li>
<p><strong>Thundering Herd Prevention</strong>: If multiple threads request the same non-resident page simultaneously, only the first thread issues a disk read. Subsequent threads wait on a page-specific <code>Mutex</code> stored in <code>inflight_loads</code>. Once the page is resident, waiters resume, avoiding redundant disk I/O. This uses a double-check lock pattern inside <code>fetch_page_*</code>.</p>
</li>
<li>
<p><strong>Lock Order Inversion Avoidance</strong>: The <code>complete_unpin</code> logic, which marks a page as evictable, is designed to run without holding any page locks. This prevents potential deadlocks that could arise from acquiring the replacer's lock while a page lock is held.</p>
</li>
<li>
<p><strong>Safe Eviction</strong>: The eviction process only selects frames with pin count zero. If a victim is dirty, WAL durability is enforced, the page is flushed, and the dirty bookkeeping cleared before reuse.</p>
</li>
<li>
<p><strong>Prefetch API</strong>: <code>prefetch_page(page_id)</code> lets callers warm the cache asynchronously. Prefetch respects the inflight table so redundant loads are coalesced.</p>
</li>
</ul>
<h2 id="4-optimizations"><a class="header" href="#4-optimizations">4. Optimizations</a></h2>
<ul>
<li>
<p><strong>Sharded LRU-K Replacer</strong>: The replacer is internally partitioned so hot pages can update their visitation history without heavy locking. Together with TinyLFU admission, it keeps the cache resilient to one-off scans.</p>
</li>
<li>
<p><strong>TinyLFU Admission Filter (Optional)</strong>: An approximate frequency-based filter that helps protect the cache from pollution caused by large, one-time scans by estimating access frequency and denying admission to "cold" pages when necessary.</p>
</li>
<li>
<p><strong>Prefetch Hooks</strong>: Iterators (e.g., B+Tree) call <code>prefetch_page</code> to stage predictable access patterns, improving hit ratios for near-future reads.</p>
</li>
<li>
<p><strong>Sequential Scan Ring Buffer (Bypass)</strong>: For full sequential scans that would otherwise thrash the cache, the iterator switches to a direct I/O ring (<code>DirectRingBuffer</code>). It reads and decodes pages through the DiskScheduler with a small in-memory window (readahead) and does not admit these pages into the main buffer pool. This significantly reduces cache pollution for large scans. Control via: <code>QUILL_STREAM_SCAN</code> (1/0), <code>QUILL_STREAM_THRESHOLD</code> (default ≈ BUFFER_POOL_SIZE/4 frames), <code>QUILL_STREAM_READAHEAD</code> (default 2 pages), and per-query planner hint <code>QUILL_STREAM_HINT</code>.</p>
</li>
<li>
<p><strong>Prefetch API</strong>: The <code>prefetch_page</code> method allows components (e.g., B+Tree iterator) to warm the cache opportunistically for predictable patterns such as short range scans.</p>
</li>
<li>
<p><strong>Flush-on-evict &amp; explicit flush</strong>: There is no background cleaner thread. Dirty pages are flushed when a victim is chosen, and the engine can call <code>flush_all_pages()</code> to guarantee durability or visibility to direct I/O paths.</p>
</li>
</ul>
<h2 id="5-benchmarking-and-performance-tuning"><a class="header" href="#5-benchmarking-and-performance-tuning">5. Benchmarking and Performance Tuning</a></h2>
<p>Effective benchmarking is crucial for evaluating and tuning the Buffer Manager. The goal is typically to measure throughput (operations per second) or latency under different workloads.</p>
<h3 id="41-example-hot-read-benchmark"><a class="header" href="#41-example-hot-read-benchmark">4.1 Example: Hot Read Benchmark</a></h3>
<p>This benchmark simulates a workload where a small, "hot" subset of data is accessed frequently. It measures the BPM's ability to keep the working set in memory.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Pseudo-code for a hot-read benchmark
use std::time::Instant;

fn benchmark_hot_reads(index: &amp;BPlusTreeIndex, total_keys: i64, num_ops: usize) {
    // 1. Identify the "hot set" (e.g., the last 10% of keys)
    let hot_set_start = (total_keys as f64 * 0.9) as i64;
    let hot_keys: Vec&lt;_&gt; = (hot_set_start..=total_keys).collect();

    // 2. Warm up the cache by reading the hot set once
    for key in &amp;hot_keys {
        let tuple = create_tuple_from_key(*key, index.key_schema.clone());
        index.get(&amp;tuple).unwrap();
    }

    // 3. Run the benchmark
    let start = Instant::now();
    for _ in 0..num_ops {
        let key_to_read = hot_keys[rand::random::&lt;usize&gt;() % hot_keys.len()];
        let tuple = create_tuple_from_key(key_to_read, index.key_schema.clone());
        // The get() call will trigger the Buffer Manager's fetch_page_read path
        index.get(&amp;tuple).unwrap();
    }
    let elapsed = start.elapsed();
    let qps = num_ops as f64 / elapsed.as_secs_f64();
    println!("Hot Read QPS: {:.2}", qps);
}
<span class="boring">}</span></code></pre></pre>
<h3 id="42-tuning-parameters"><a class="header" href="#42-tuning-parameters">4.2 Tuning Parameters</a></h3>
<ul>
<li><strong>Environment Variables (selected)</strong>:
<ul>
<li><code>QUILL_STREAM_SCAN</code> / <code>QUILL_STREAM_THRESHOLD</code> / <code>QUILL_STREAM_READAHEAD</code> / <code>QUILL_STREAM_HINT</code></li>
</ul>
</li>
<li><strong>Admission Policy</strong>: TinyLFU can be used to prevent one-time scans from polluting the pool. Adjust its aggressiveness based on workload characteristics.</li>
<li><strong>Streaming Scan</strong>: Ensure that large sequential scans no longer degrade hit ratios. Compare QPS/latency with and without the ring buffer, and tune <code>QUILL_STREAM_READAHEAD</code> (typically 2–8).</li>
<li><strong>Buffer Pool Size</strong>: The most critical lever. Too small → frequent evictions and poor performance. Size it to the expected hot working set.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="disk-io--scheduler-io_uring-data-pages--wal-runtime"><a class="header" href="#disk-io--scheduler-io_uring-data-pages--wal-runtime">Disk I/O — Scheduler, io_uring Data Pages &amp; WAL Runtime</a></h1>
<h2 id="1-architecture"><a class="header" href="#1-architecture">1. Architecture</a></h2>
<ul>
<li><strong>Request Path</strong>: foreground components enqueue <code>DiskRequest</code> objects via <code>DiskScheduler::{schedule_read, schedule_write, …}</code>. A dispatcher thread drains the global channel and distributes work round-robin to N io_uring workers. Each worker owns its own ring and file-descriptor cache, so once a request is forwarded, execution proceeds entirely off the foreground thread.</li>
<li><strong>Stable APIs</strong>: <code>schedule_read(page_id)</code>, <code>schedule_write(page_id, Bytes)</code>, <code>schedule_read_pages(Vec&lt;PageId&gt;)</code>, <code>schedule_allocate()</code>, <code>schedule_deallocate(page_id)</code> — every call returns a channel the caller can block on or poll.</li>
<li><strong>Batch Reads</strong>: <code>ReadPages</code> fans out per-page SQEs while a shared <code>BatchState</code> tracks completions. Even if the kernel completes I/O out of order, the caller receives a <code>Vec&lt;BytesMut&gt;</code> that preserves the original page order.</li>
</ul>
<h2 id="2-wal-runtime-buffered-io"><a class="header" href="#2-wal-runtime-buffered-io">2. WAL Runtime (buffered I/O)</a></h2>
<ul>
<li>Dedicated WAL runtime threads handle sequential WAL appends/reads using buffered I/O. They now keep a per-thread cache of open segment files, eliminating repeated <code>open()</code>/<code>close()</code> on every log record.</li>
<li>Worker count defaults to <code>max(1, available_parallelism / 2)</code> but is tunable through <code>IOSchedulerConfig</code>.</li>
<li>Optional <code>sync</code> on a request triggers <code>sync_data</code> / <code>fdatasync</code> so <code>WalManager</code> can honour synchronous commit or checkpoint barriers. Data pages stay on the io_uring dataplane; WAL always uses buffered writes.</li>
</ul>
<h2 id="3-io_uring-backend-linux"><a class="header" href="#3-io_uring-backend-linux">3. io_uring Backend (Linux)</a></h2>
<ul>
<li>Each worker owns an <code>IoUring</code> with configurable <code>queue_depth</code>, optional SQPOLL idle timeout, and a pool of registered fixed buffers sized to <code>PAGE_SIZE</code>. Workers submit SQEs asynchronously and drain CQEs in small batches to keep the ring warm.</li>
<li>Read batching relies on shared <code>BatchState</code> instances (<code>Rc&lt;RefCell&lt;_&gt;&gt;</code>) so multi-page callers see ordered results without blocking the kernel on serialization.</li>
<li>Writes keep their payload alive until completion; if a fixed buffer slot is available we reuse it, otherwise we fall back to heap buffers. A companion <code>WriteState</code> tracks an optional <code>fdatasync</code> so the caller still observes exactly one <code>Result&lt;()&gt;</code> once all CQEs land.</li>
<li>Errors (short read/write, errno) are normalised into <code>QuillSQLError</code> values that flow back on the original channel.</li>
</ul>
<h2 id="4-configuration"><a class="header" href="#4-configuration">4. Configuration</a></h2>
<ul>
<li><code>config::IOSchedulerConfig</code> controls:
<ul>
<li><code>workers</code>: number of io_uring workers (default = available parallelism).</li>
<li><code>wal_workers</code>: WAL runtime threads (default workers / 2).</li>
<li><code>iouring_queue_depth</code>, <code>iouring_fixed_buffers</code>, <code>iouring_sqpoll_idle_ms</code>.</li>
<li><code>fsync_on_write</code>: whether data-page writes also issue <code>fdatasync</code> (WAL sync is managed separately by <code>WalManager</code>).</li>
</ul>
</li>
</ul>
<h2 id="5-concurrency--safety"><a class="header" href="#5-concurrency--safety">5. Concurrency &amp; Safety</a></h2>
<ul>
<li>Worker-local file descriptors plus positional I/O remove shared mutable state on the hot path. The new per-worker handle cache further reduces syscall overhead.</li>
<li>Shutdown sequence: enqueue <code>Shutdown</code>, dispatcher forwards it to every worker, each worker drains outstanding SQEs/CQEs, and finally dispatcher + workers are joined.</li>
<li>BufferPool, TableHeap, and the streaming scan ring buffer still integrate via channels; inflight guards prevent duplicate page fetches.</li>
</ul>
<h2 id="6-performance-notes"><a class="header" href="#6-performance-notes">6. Performance Notes</a></h2>
<ul>
<li>Random page access benefits from fewer syscalls and deeper outstanding queue depth than the blocking fallback.</li>
<li>Only the io_uring backend currently ships (Linux x86_64). A portable fallback remains future work.</li>
<li>For large sequential scans, combine <code>ReadPages</code> with the ring-buffer iterator to minimise buffer-pool churn.</li>
</ul>
<h2 id="7-future-work"><a class="header" href="#7-future-work">7. Future Work</a></h2>
<ul>
<li>Queue-depth aware scheduling and CQE bulk harvesting.</li>
<li>Optional group commit (aggregate writes, single fsync) behind configuration.</li>
<li>Metrics hooks (queue depth, submit/complete throughput, latency percentiles, error codes).</li>
<li>Cross-platform fallback backend and richer prioritisation/throttling policies.</li>
<li>Control-plane knobs for throttling individual background workers.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="write-ahead-logging-wal--architecture-and-recovery"><a class="header" href="#write-ahead-logging-wal--architecture-and-recovery">Write-Ahead Logging (WAL) — Architecture and Recovery</a></h1>
<p>This document details the architecture of QuillSQL's Write-Ahead Logging (WAL) system, which is essential for ensuring crash recovery, data durability, and atomicity (the 'A' and 'D' in ACID).</p>
<h2 id="1-high-level-overview"><a class="header" href="#1-high-level-overview">1. High-Level Overview</a></h2>
<p>The core principle of WAL is simple but powerful: <strong>before any data page modification is written to the main database file, a log record describing that change must first be written and persisted to a separate log file.</strong></p>
<p>This approach provides two primary benefits:</p>
<ol>
<li><strong>Durability &amp; Atomicity</strong>: If the system crashes, we can replay the log to restore the database to a consistent state, redoing committed changes and undoing incomplete ones.</li>
<li><strong>Performance</strong>: It transforms many small, random disk writes (updating various pages in the data file) into a single, highly efficient sequential append to the log file. The actual data file pages can then be flushed to disk in the background in a more optimized manner.</li>
</ol>
<h2 id="2-architecture--components"><a class="header" href="#2-architecture--components">2. Architecture &amp; Components</a></h2>
<p>Our WAL implementation is inspired by the ARIES recovery algorithm and consists of several key components that work in concert.</p>
<pre><code>+-------------------+   writes   +-----------------+   queued    +-----------------------+
| Execution Engine  |-----------&gt;|   WalManager    |-----------&gt; | DiskScheduler/io_uring |
| (e.g., TableHeap) |            | (in-memory buf) |   (MPSC)    | (shared worker pool)  |
+-------------------+            +-----------------+             +-----------------------+
        |                                  |                              |
(modifies pages)                       (flushes)                     (reads/writes)
        v                                  v                              v
+-------------------+  (dirty pages) +---------------+            +----------------+
|  Buffer Pool Mgr  |&lt;-------------&gt;|  Checkpointer |-----------&gt;|   WAL Files    |
+-------------------+                +---------------+            +----------------+
        |  (flushes pages)                                            ^
        v                                                             |
+-------------------+                +-------------------+            |
|  Data Files (.db) |                |  RecoveryManager  |&lt;-----------+
+-------------------+                +-------------------+
</code></pre>
<ul>
<li>
<p><strong><code>WalManager</code> (<code>src/recovery/wal.rs</code>)</strong>: The central coordinator of the WAL system.</p>
<ul>
<li><strong>Responsibilities</strong>: Assigns unique Log Sequence Numbers (LSNs), manages an in-memory log buffer, and provides the primary interface (<code>append_record_with</code>) for other parts of the system to write log records. It also tracks the <code>durable_lsn</code>—the LSN up to which logs have been successfully flushed to disk.</li>
<li><strong>FPW (First-Page-Write)</strong>: Tracks pages modified since the last checkpoint to enforce a full-page write for the first modification, protecting against torn-page scenarios during recovery.</li>
<li><strong>I/O Runtime Integration</strong>: <code>WalManager</code> now submits WAL writes directly to the shared <code>DiskScheduler</code> io_uring workers. Each flush request enqueues sequential writes (and optional <code>fdatasync</code>) and waits for the corresponding completion events to guarantee durability ordering.</li>
</ul>
</li>
<li>
<p><strong><code>Checkpointer</code> (Background Thread)</strong>: A periodic process crucial for bounding recovery time.</p>
<ul>
<li><strong>Responsibilities</strong>:
<ol>
<li>Writes a <code>Checkpoint</code> record to the WAL. This record contains a snapshot of the current state, including the list of active transactions (ATT) and the Dirty Page Table (DPT). The DPT lists all pages that are dirty in the buffer pool, along with the LSN of the log record that first made them dirty (<code>recLSN</code>).</li>
<li>After a checkpoint is durable, it triggers the recycling of old, no-longer-needed WAL segment files, preventing infinite log growth.</li>
</ol>
</li>
</ul>
</li>
<li>
<p><strong><code>RecoveryManager</code> (<code>src/recovery/recovery_manager.rs</code>)</strong>: The engine that performs crash recovery upon database startup. It implements a three-phase recovery process and talks to the shared <code>DiskScheduler</code> (backed by io_uring) for data-page redo/undo. WAL replay and data I/O remain decoupled but reside in the same module for tighter invariants.</p>
</li>
<li>
<p><strong><code>ControlFile</code> (<code>src/recovery/control_file.rs</code>)</strong>: A small, critical file (<code>control.dat</code>) that bootstraps the recovery process. It stores essential metadata like the system identifier, WAL segment size, and, most importantly, the LSN of the last successful checkpoint.</p>
</li>
</ul>
<h2 id="3-log-record-format"><a class="header" href="#3-log-record-format">3. Log Record Format</a></h2>
<p>All log records are encapsulated in a <code>WalFrame</code> and written to disk. The structure is defined in <code>src/recovery/wal_record.rs</code>.</p>
<ul>
<li><strong><code>WalFrame</code></strong>: Contains a header with metadata (<code>lsn</code>, <code>prev_lsn</code>, CRC for integrity) and a <code>WalRecordPayload</code>.</li>
<li><strong><code>WalRecordPayload</code></strong>: An enum representing the actual log content. We use a mix of logging types:
<ul>
<li><strong>Physical/Physiological Logs (for Redo)</strong>:
<ul>
<li><code>PageWrite</code>: A full 4KB image of a page. Used for FPW.</li>
<li><code>PageDelta</code>: Records only the changed byte range within a page (<code>offset</code>, <code>data</code>). This is much more efficient for subsequent writes to the same page.</li>
</ul>
</li>
<li><strong>Logical Logs (for Undo)</strong>:
<ul>
<li><code>HeapInsert</code>, <code>HeapUpdate</code>, <code>HeapDelete</code>: These records describe the logical operation performed on the table heap. For example, <code>HeapUpdate</code> contains both the new and old versions of the tuple, allowing for a precise rollback. Each record contains the <code>op_txn_id</code> to identify which transaction performed the action.</li>
</ul>
</li>
<li><strong>Control Flow Logs</strong>:
<ul>
<li><code>Transaction</code>: Records <code>Begin</code>, <code>Commit</code>, and <code>Abort</code> events for transactions.</li>
<li><code>Checkpoint</code>: The record written by the <code>Checkpointer</code>.</li>
<li><code>Clr (Compensation Log Record)</code>: A special logical log written during the Undo phase to record that an operation has been undone. This ensures that if the system crashes during recovery, Undo operations are not applied more than once (idempotence).</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="4-key-processes"><a class="header" href="#4-key-processes">4. Key Processes</a></h2>
<h3 id="41-normal-operation-eg-an-update"><a class="header" href="#41-normal-operation-eg-an-update">4.1 Normal Operation (e.g., an <code>UPDATE</code>)</a></h3>
<ol>
<li><strong>Logical Log</strong>: The <code>TableHeap</code> first modifies a page in the buffer pool. It then immediately generates a logical <code>HeapUpdate</code> record containing the old and new tuple data and sends it to the <code>WalManager</code>.</li>
<li><strong>Physical/Physiological Log</strong>: When the <code>WritePageGuard</code> for the modified page is dropped, the buffer pool triggers a write-back hook. This hook calculates the physical changes to the page and generates either a <code>PageWrite</code> (on first touch) or a <code>PageDelta</code> record, which is also sent to the <code>WalManager</code>. The page's own LSN (<code>page_lsn</code>) is updated to match that of this physical log record.</li>
</ol>
<h3 id="42-transaction-commit"><a class="header" href="#42-transaction-commit">4.2 Transaction Commit</a></h3>
<ul>
<li>A <code>Transaction(Commit)</code> record is written to the WAL.</li>
<li>If <code>synchronous_commit</code> is enabled, the transaction thread will block until the <code>WalManager</code> confirms that the commit record's LSN is durable (flushed to disk).</li>
</ul>
<h3 id="43-crash-recovery-process"><a class="header" href="#43-crash-recovery-process">4.3 Crash Recovery Process</a></h3>
<p>Upon restart, <code>RecoveryManager::replay()</code> is executed:</p>
<ol>
<li>
<p><strong>Analysis Phase</strong>:</p>
<ul>
<li>It starts from the last checkpoint recorded in <code>control.dat</code> and scans the WAL forward to the end.</li>
<li>It reconstructs the state at the moment of the crash: the set of uncommitted transactions (the "losers") and the dirty page table (DPT).</li>
</ul>
</li>
<li>
<p><strong>Redo Phase</strong>:</p>
<ul>
<li>It finds the smallest <code>recLSN</code> from the DPT. This is the earliest point from which modifications might have been lost.</li>
<li>It scans forward from this <code>recLSN</code>, reapplying all physical and physiological changes (<code>PageWrite</code>, <code>PageDelta</code>) to the data pages. This phase is idempotent and brings the database to its exact state at the time of the crash, including changes from "loser" transactions. CLR records are ignored.</li>
</ul>
</li>
<li>
<p><strong>Undo Phase</strong>:</p>
<ul>
<li>It scans the WAL backward from the end.</li>
<li>For each log record belonging to a "loser" transaction (identified via <code>op_txn_id</code>), it performs the corresponding logical inverse operation (e.g., for a <code>HeapInsert</code>, it deletes the tuple).</li>
<li>Crucially, after performing an undo action, it writes a <strong>CLR</strong> to the WAL. This CLR points to the LSN of the record that was just undone. If a crash happens during Undo, upon restart, the recovery process will see the CLR during the Redo phase and know not to re-apply the already-undone operation.</li>
</ul>
</li>
</ol>
<h2 id="5-configuration"><a class="header" href="#5-configuration">5. Configuration</a></h2>
<p>The WAL system's behavior can be tuned via <code>WalOptions</code> in <code>src/database.rs</code>, which can be set via command-line arguments or environment variables. Key parameters include:</p>
<ul>
<li><code>wal_segment_size</code>: The size of individual log files.</li>
<li><code>wal_writer_interval_ms</code>: The wake-up interval for the background <code>WalWriter</code>.</li>
<li><code>wal_checkpoint_interval_ms</code>: The frequency of checkpoints.</li>
<li><code>synchronous_commit</code>: Whether commits wait for disk durability.</li>
</ul>
<h2 id="6-transaction-manager-integration"><a class="header" href="#6-transaction-manager-integration">6. Transaction Manager Integration</a></h2>
<p>The WAL stack is tightly coupled with the transaction subsystem:</p>
<ul>
<li><code>TransactionManager::begin</code> writes a <code>Transaction(Begin)</code> record and stamps the transaction's <code>begin_lsn</code>.</li>
<li><code>commit</code> appends <code>Transaction(Commit)</code> and blocks on durability when <code>synchronous_commit</code> is enabled; async mode still calls <code>flush_until</code>.</li>
<li><code>abort</code> walks the transaction's undo stack, emits CLRs + logical undo records, then appends <code>Transaction(Abort)</code>.</li>
<li>Executors record undo actions (insert/update/delete) so abort/rollback can replay the logical inverse.</li>
<li>Isolation/access modes (<code>ReadUncommitted</code>, <code>ReadCommitted</code>, <code>RepeatableRead</code>, <code>Serializable</code>, <code>READ ONLY</code>) are managed in <code>SessionContext</code> and enforced by the planner/execution. <code>READ ONLY</code> transactions fail early on DML via <code>ExecutionContext::ensure_writable</code>, but still participate in WAL for consistency (BEGIN/COMMIT records).</li>
</ul>
<h2 id="7-observability--logging"><a class="header" href="#7-observability--logging">7. Observability &amp; Logging</a></h2>
<ul>
<li>Lock manager adds trace logs (<code>lock granted</code>, <code>wait edge</code>) and WARN on deadlock detection, assisting investigation of blocking scenarios.</li>
<li>WAL exposes <code>durable_lsn</code>, <code>max_assigned_lsn</code>, and background flush status.</li>
<li>Integration tests in <code>transaction_tests.rs</code> cover read-only rejection, RC update visibility, and RR blocking behavior.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transactions-concurrency--mvcc"><a class="header" href="#transactions-concurrency--mvcc">Transactions, Concurrency &amp; MVCC</a></h1>
<p>This guide dives into QuillSQL’s transaction subsystem, how MVCC snapshots are produced and consumed, and the supporting background processes that keep old versions in check. Diagrams are provided using Mermaid for easy rendering.</p>
<hr />
<h2 id="1-module-overview"><a class="header" href="#1-module-overview">1. Module Overview</a></h2>
<pre><code class="language-mermaid">flowchart LR
    Session["SessionContext"] --&gt; TM["TransactionManager"]
    TM --&gt; LM["LockManager"]
    TM --&gt; WAL["WalManager"]
    TM --&gt; Snap["TransactionSnapshot"]
    TM --&gt; Undo["Undo stack"]
    LM --&gt;|table/row locks| Storage["TableHeap / Index"]
    Snap --&gt; Runtime["TxnRuntime"]
    Runtime --&gt; Exec["Physical operators"]
</code></pre>
<ul>
<li>
<p><strong><code>SessionContext</code> (<code>src/session/mod.rs</code>)</strong><br />
Manages per-connection defaults (isolation, access mode, autocommit) and the active <code>Transaction</code>. Handles <code>SET TRANSACTION</code>/<code>SET SESSION TRANSACTION</code>, ensuring new transactions inherit the desired modes.</p>
</li>
<li>
<p><strong><code>Transaction</code> (<code>src/transaction/transaction.rs</code>)</strong><br />
Stores the transaction id, isolation/access mode, state, WAL LSNs, undo actions, command counters, and—starting with this release—a cached MVCC snapshot used for Repeatable Read / Serializable.</p>
</li>
<li>
<p><strong><code>TransactionManager</code></strong><br />
Entry point for <code>begin</code>, <code>commit</code>, and <code>abort</code>. Assigns IDs, writes WAL begin/commit/abort records, maintains active transaction sets, and provides helper methods for acquiring table/row locks. Produces <code>TransactionSnapshot</code> objects that encode <code>xmin</code>, <code>xmax</code>, and the active transaction list for visibility decisions.</p>
</li>
<li>
<p><strong><code>TxnRuntime</code> (<code>src/transaction/runtime.rs</code>)</strong><br />
Lightweight wrapper handed to physical operators. Each command increments the transaction’s command id, selects (or reuses) the appropriate snapshot based on isolation level, and exposes helpers for row/table locks plus visibility checks.</p>
</li>
<li>
<p><strong><code>LockManager</code></strong><br />
Central 2PL engine with multi-granularity modes (IS/IX/S/SIX/X), FIFO wait queues, and deadlock detection via a wait-for graph. Shared row locks are released eagerly for RC, retained within the command for RR, and held to commit for Serializable.</p>
</li>
<li>
<p><strong>Undo Stack</strong><br />
DML operators push logical undo entries so abort/rollback can restore heap pages and remove secondary index entries. Abort writes CLRs and replays undo actions.</p>
</li>
<li>
<p><strong>Background MVCC Vacuum</strong><br />
<code>background::spawn_mvcc_vacuum_worker</code> computes <code>safe_xmin</code> from the transaction manager’s oldest active id and reclaims tuple versions deleted by committed transactions or inserted by aborted ones once no running transaction can see them.</p>
</li>
</ul>
<hr />
<h2 id="2-snapshot-semantics"><a class="header" href="#2-snapshot-semantics">2. Snapshot Semantics</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Isolation Level</th><th>Snapshot Lifetime</th><th>Lock Behavior</th><th>Notes</th></tr></thead><tbody>
<tr><td>Read Uncommitted</td><td>Fresh snapshot per command</td><td>No shared row locks</td><td>May see dirty reads; used mainly for diagnostics.</td></tr>
<tr><td>Read Committed</td><td>Fresh snapshot per command</td><td>Shared locks acquired then released immediately</td><td>Prevents dirty reads; each statement sees the latest committed data.</td></tr>
<tr><td>Repeatable Read</td><td>Snapshot cached at first command and reused</td><td>Shared locks kept for the duration of the command, then released</td><td>Provides statement-consistent reads. MVCC ensures previously read rows remain stable across statements.</td></tr>
<tr><td>Serializable</td><td>Same cached snapshot as RR</td><td>Shared locks retained until commit (strict 2PL)</td><td>Today equivalent to locking-based serializable. Future work: SSI / predicate locking.</td></tr>
</tbody></table>
</div>
<ul>
<li><code>Transaction::set_snapshot</code> caches the snapshot for RR/SR; <code>clear_snapshot</code> is invoked when the isolation level drops to RC/RU or at commit/abort.</li>
<li><code>TransactionSnapshot::is_visible</code> inspects tuple metadata (<code>insert_txn_id</code>, <code>delete_txn_id</code>, <code>CommandId</code>) to decide whether the tuple should be visible for the current command, accounting for aborted transactions, active transactions, and self-modifications.</li>
</ul>
<hr />
<h2 id="3-statement-execution-timeline"><a class="header" href="#3-statement-execution-timeline">3. Statement Execution Timeline</a></h2>
<pre><code class="language-mermaid">sequenceDiagram
    participant Client
    participant Session
    participant Exec as Executor
    participant Runtime as TxnRuntime
    participant Heap as TableHeap
    participant TM as TransactionManager

    Client-&gt;&gt;Session: "BEGIN ISOLATION LEVEL REPEATABLE READ"
    Session-&gt;&gt;TM: begin(RR, ReadWrite)
    TM--&gt;&gt;Session: Transaction { snapshot: None }
    Client-&gt;&gt;Session: "SELECT * FROM accounts"
    Session-&gt;&gt;Exec: plan + txn
    Exec-&gt;&gt;Runtime: TxnRuntime::new(&amp;TM, &amp;mut txn)
    Runtime--&gt;&gt;Exec: snapshot S0, command_id 0
    Exec-&gt;&gt;Heap: SeqScan (S locks retained for this command)
    Heap--&gt;&gt;Exec: (rid, meta, tuple)
    Exec-&gt;&gt;Runtime: is_visible(meta)?
    Runtime--&gt;&gt;Exec: decision based on S0
    Client-&gt;&gt;Session: "UPDATE accounts SET balance=... WHERE id=1"
    Exec-&gt;&gt;Runtime: TxnRuntime::new(&amp;TM, &amp;mut txn)
    Runtime--&gt;&gt;Exec: reuse snapshot S0, command_id 1
    Exec-&gt;&gt;Heap: mvcc_update + exclusive lock
    Exec-&gt;&gt;TM: register undo + WAL
    Client-&gt;&gt;Session: "COMMIT"
    Session-&gt;&gt;TM: commit(&amp;mut txn)
    TM-&gt;&gt;WAL: Transaction(Commit)
    TM--&gt;&gt;Session: release locks, clear snapshot
</code></pre>
<p>Key behaviors:</p>
<ul>
<li>UPDATE skips tuple versions created by the same <code>(txn_id, command_id)</code> to avoid chasing its own MVCC chain.</li>
<li>If another transaction commits in between the two statements, its new version remains invisible to the RR transaction thanks to the cached snapshot.</li>
<li>Shared locks used to verify visibility are released at the end of each command in RR, while Serializable holds them longer to ensure strict 2PL.</li>
</ul>
<hr />
<h2 id="4-background-mvcc-vacuum"><a class="header" href="#4-background-mvcc-vacuum">4. Background MVCC Vacuum</a></h2>
<pre><code class="language-mermaid">flowchart TD
    TM["TransactionManager"] --&gt;|oldest_active_txn| Vacuum
    Vacuum["background::mvcc_vacuum_worker"] --&gt;|safe_xmin| Compact
    Compact["vacuum_table_versions"] --&gt; Table["TableHeap"]
</code></pre>
<p>Algorithm sketch:</p>
<ol>
<li>Compute <code>safe_xmin = oldest_active_txn() or next_txn_id_hint()</code>.</li>
<li>For each registered table (excluding information_schema), iterate tuples in batches.</li>
<li>If a tuple is marked deleted and <code>delete_txn_id &lt; safe_xmin</code>, reclaim it.</li>
<li>If a tuple was inserted by an aborted txn with <code>insert_txn_id &lt; safe_xmin</code>, reclaim it.</li>
<li>Stop after <code>batch_limit</code> tuples per run.</li>
</ol>
<p>This keeps heap pages compact over time without blocking foreground work.</p>
<hr />
<h2 id="5-integration-with-wal--recovery"><a class="header" href="#5-integration-with-wal--recovery">5. Integration with WAL &amp; Recovery</a></h2>
<ul>
<li><code>TransactionManager::begin/commit/abort</code> append WAL records. Commit optionally waits for durability based on <code>synchronous_commit</code>.</li>
<li>Aborts walk the undo stack, emit CLRs, and replay inverse heap/index changes.</li>
<li>Recovery follows ARIES (analysis → redo → undo). CLRs make undo idempotent in case a crash happens during recovery.</li>
<li>Buffer manager tracks dirty pages and recLSNs; checkpoints capture the ATT + DPT so redo can start near the earliest outstanding change.</li>
</ul>
<hr />
<h2 id="6-testing--diagnostics"><a class="header" href="#6-testing--diagnostics">6. Testing &amp; Diagnostics</a></h2>
<ul>
<li>Unit tests in <code>src/tests/transaction_tests.rs</code> cover:
<ul>
<li>Read-only enforcement (<code>read_only_transaction_rejects_dml</code>).</li>
<li>Read Committed self-update (<code>read_committed_allows_update_after_select</code>).</li>
<li>Lock blocking (<code>repeatable_read_blocks_update_until_commit</code>).</li>
<li>Snapshot isolation (<code>repeatable_read_sees_consistent_snapshot_after_update</code>).</li>
</ul>
</li>
<li>Use <code>RUST_LOG=trace</code> to surface lock grants, wait edges, and deadlock warnings.</li>
<li><code>TransactionManager::active_transactions()</code> and <code>oldest_active_txn()</code> help debugging vacuum decisions.</li>
</ul>
<hr />
<h2 id="7-future-work-1"><a class="header" href="#7-future-work-1">7. Future Work</a></h2>
<ul>
<li>Predicate / next-key locking to prevent phantoms under Serializable.</li>
<li>Expose transaction statistics (wait time, undo size) via system tables.</li>
<li>Smarter vacuum scheduling based on table size and churn.</li>
<li>Serializable Snapshot Isolation (SSI) to reduce locking while ensuring serializability.</li>
</ul>
<p>The current system already mirrors industrial designs: MVCC + 2PL, ARIES-style WAL, cached snapshots for higher isolation levels, and background vacuuming. These docs should help both contributors and students trace how a transaction traverses the stack and where to hook in new behavior.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="b-tree-index--architecture-and-concurrency"><a class="header" href="#b-tree-index--architecture-and-concurrency">B+ Tree Index — Architecture and Concurrency</a></h1>
<h2 id="1-architecture-overview-1"><a class="header" href="#1-architecture-overview-1">1. Architecture Overview</a></h2>
<h3 id="11-node-and-page-structure"><a class="header" href="#11-node-and-page-structure">1.1 Node and Page Structure</a></h3>
<ul>
<li><strong>Node Types</strong>: <code>Internal</code> and <code>Leaf</code>.
<ul>
<li><strong>Internal Nodes</strong>: Store separator keys and pointers to child pages. The first pointer is a "sentinel" that points to the subtree for all keys less than the first key in the node.</li>
<li><strong>Leaf Nodes</strong>: Store <code>(key, RecordId)</code> pairs in sorted order. Leaves are linked together in a singly linked list (<code>next_page_id</code>) to allow for efficient range scans.</li>
</ul>
</li>
<li><strong>Header Page</strong>: A fixed page (<code>header_page_id</code>) that acts as the entry point to the tree. It stores the <code>root_page_id</code>, allowing for atomic updates to the tree root when it splits or shrinks.</li>
<li><strong>Page Layout</strong>:
<ul>
<li><strong>Internal Page</strong>: <code>{Header, High Key, Array&lt;(Key, ChildPageId)&gt;}</code>. The <code>High Key</code> is part of the B-link optimization.</li>
<li><strong>Leaf Page</strong>: <code>{Header, Array&lt;(Key, RID)&gt;}</code>.</li>
</ul>
</li>
</ul>
<h3 id="12-b-link-structure"><a class="header" href="#12-b-link-structure">1.2 B-link Structure</a></h3>
<p>The tree uses B-link pointers (<code>next_page_id</code>) on all levels (both internal and leaf nodes). This creates a "side-link" to the right sibling. This is crucial for concurrency, as it allows readers to recover from transient inconsistent states caused by concurrent page splits by "chasing" the link to the right sibling.</p>
<pre><code>              +------------------+
              |   Root (Int)     |
              +------------------+
             /         |         \
   +--------+      +--------+      +--------+
   | Int P1 |-----&gt;| Int P2 |-----&gt;| Int P3 |  (Internal B-link pointers)
   +--------+      +--------+      +--------+
   /   |   \      /   |   \      /   |   \
+----+ +----+  +----+ +----+  +----+ +----+
| L1 |-| L2 |-&gt;| L3 |-| L4 |-&gt;| L5 |-| L6 | (Leaf chain / B-links)
+----+ +----+  +----+ +----+  +----+ +----+
</code></pre>
<h2 id="2-concurrency-control"><a class="header" href="#2-concurrency-control">2. Concurrency Control</a></h2>
<p>The B+Tree employs a sophisticated, lock-free read path and a high-concurrency write path using latch crabbing.</p>
<h3 id="21-read-path-optimistic-lock-coupling-olc-with-b-links"><a class="header" href="#21-read-path-optimistic-lock-coupling-olc-with-b-links">2.1 Read Path: Optimistic Lock Coupling (OLC) with B-links</a></h3>
<ul>
<li>Readers traverse the tree from the root without taking any locks.</li>
<li>On each page, a <code>version</code> number is read before and after processing the page's contents. If the version changes, it indicates a concurrent modification, and the read operation restarts from the root.</li>
<li>If a reader is traversing an internal node and the search key is greater than or equal to the node's <code>high_key</code>, it knows a split has occurred. Instead of restarting, it can use the <code>next_page_id</code> B-link to "chase" to the right sibling and continue the search, minimizing restarts.</li>
</ul>
<h3 id="22-write-path-latch-crabbing"><a class="header" href="#22-write-path-latch-crabbing">2.2 Write Path: Latch Crabbing</a></h3>
<p>Writers (insert/delete) use a technique called <strong>latch crabbing</strong> (or lock coupling) to ensure safe concurrent modifications.</p>
<ul>
<li><strong>Process</strong>: A writer acquires a write latch on a parent node before fetching and latching a child node. Once the child is latched, the writer checks if the child is "safe" for the operation (i.e., not full for an insert, not at minimum size for a delete).
<ul>
<li>If the child is <strong>safe</strong>, the latch on the parent (and all other ancestors) is released.</li>
<li>If the child is <strong>unsafe</strong>, the parent latch is held, as the child might need to split or merge, which would require modifying the parent.</li>
</ul>
</li>
<li><strong><code>Context</code> Struct</strong>: This process is managed by a <code>Context</code> struct that holds the stack of write guards (<code>write_set</code>) for the current traversal path. Releasing ancestor latches is as simple as clearing this stack.</li>
</ul>
<pre><code>Latch Crabbing on Insert:
1. Latch(Root)
2. Descend to Child C1. Latch(C1).
3. Check if C1 is safe (not full).
   IF SAFE:
     ReleaseLatch(Root). Path is now just {C1}.
   IF UNSAFE (full):
     Keep Latch(Root). Path is {Root, C1}.
4. Descend from C1 to C2. Latch(C2).
5. Check if C2 is safe... and so on.
</code></pre>
<h3 id="23-deadlock-avoidance"><a class="header" href="#23-deadlock-avoidance">2.3 Deadlock Avoidance</a></h3>
<p>When modifying siblings (during merge or redistribution), deadlocks are possible if two threads try to acquire latches on the same two pages in opposite orders. This is prevented by enforcing a strict <strong>PageId-ordered locking</strong> protocol. When two sibling pages must be latched, the page with the lower <code>PageId</code> is always latched first.</p>
<h2 id="3-key-algorithms--features"><a class="header" href="#3-key-algorithms--features">3. Key Algorithms &amp; Features</a></h2>
<ul>
<li><strong>Parent-Guided Redirection</strong>: During an insert or delete, after a writer has descended to a leaf, it re-validates its position using the parent (if a latch is still held). If a concurrent split has moved the target key range to a different sibling, the writer can jump directly to the correct page instead of traversing the leaf chain, preventing race conditions.</li>
<li><strong>Iterator</strong>: The iterator performs a forward scan by following the leaf chain (<code>next_page_id</code>). It uses a lightweight form of OLC, checking the leaf page version to detect concurrent modifications and restart if necessary to ensure it doesn't miss keys.
<ul>
<li>Sequential Scan Optimization (RingBuffer): For large range scans, the iterator switches to a "synchronous batch fetch + local ring buffer" mode. It fills a small <code>RingBuffer&lt;BytesMut&gt;</code> with consecutive leaf pages (by following <code>next_page_id</code>) and then decodes KVs locally without holding page guards for long. This reduces buffer pool pollution and syscall/lock overhead.</li>
<li>Two Iteration Modes:
<ul>
<li>Guard Mode: Keep a <code>ReadPageGuard</code> and decode per step; prefetch next leaf best-effort.</li>
<li>Bytes Mode: After switching, decode from <code>BytesMut</code> buffers in the local ring; when a leaf is exhausted, pop next bytes from the ring or refill by following the chain.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Prefix Compression</strong>: Keys in internal nodes are prefix-compressed to save space. Each key is stored as <code>(lcp, suffix_len, suffix)</code>. This reduces the size of internal pages, increasing the tree's fanout and reducing its height, which improves cache performance and reduces I/O.</li>
<li><strong>Split/Merge Safety</strong>:
<ul>
<li><strong>Split</strong>: When a node splits, the new right sibling is written first. Then, the B-link pointer and separator key are published atomically by updating the left sibling and parent. This ensures readers can always navigate the structure correctly.</li>
<li><strong>Merge/Redistribute</strong>: When a node is underfull, the implementation first tries to borrow an entry from a sibling (redistribute). If both siblings are at minimum size, it merges with a sibling. All these operations carefully maintain the B-link chain and parent pointers.</li>
</ul>
</li>
</ul>
<h2 id="4-benchmarks--performance"><a class="header" href="#4-benchmarks--performance">4. Benchmarks &amp; Performance</a></h2>
<h3 id="41-example-range-scan-benchmark"><a class="header" href="#41-example-range-scan-benchmark">4.1 Example: Range Scan Benchmark</a></h3>
<p>This benchmark measures the efficiency of the leaf-chain traversal, which is critical for <code>SELECT</code> queries with <code>WHERE</code> clauses on indexed columns. It benefits from iterator prefetching and prefix compression.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Pseudo-code for a range scan benchmark
use std::time::Instant;

fn benchmark_range_scan(index: Arc&lt;BPlusTreeIndex&gt;, num_keys: i64, num_passes: usize) {
    // 1. Populate the index with sequential keys
    for key in 1..=num_keys {
        let tuple = create_tuple_from_key(key, index.key_schema.clone());
        index.insert(&amp;tuple, create_rid_from_key(key)).unwrap();
    }

    // 2. Run the benchmark
    let start = Instant::now();
    let mut count = 0;
    for _ in 0..num_passes {
        // Create an iterator over the full key range
        let mut iter = TreeIndexIterator::new(index.clone(), ..);
        while let Some(_) = iter.next().unwrap() {
            count += 1;
        }
    }
    let elapsed = start.elapsed();
    let total_items_scanned = num_keys as usize * num_passes;
    let items_per_sec = total_items_scanned as f64 / elapsed.as_secs_f64();

    println!(
        "Range Scan: Scanned {} items in {:?}. Throughput: {:.2} items/sec",
        total_items_scanned, elapsed, items_per_sec
    );
}
<span class="boring">}</span></code></pre></pre>
<h3 id="42-performance-notes"><a class="header" href="#42-performance-notes">4.2 Performance Notes</a></h3>
<ul>
<li><strong>Hot Reads</strong>: Performance on hot-spot reads depends on keeping upper levels and hot leaves resident in the buffer pool. Warm up the cache for read-heavy benchmarks. Protect hot pages from pollution by enabling TinyLFU admission.</li>
<li><strong>Large Range Scans</strong>: Prefer table SeqScan with ring buffer (bypass) when scanning most of the table. For index scans over very large ranges, consider future Bitmap Heap Scan rather than bypassing the pool for leaves.</li>
</ul>
<h3 id="43-configuration"><a class="header" href="#43-configuration">4.3 Configuration</a></h3>
<ul>
<li><code>config::BTreeConfig</code> controls iterator behavior:
<ul>
<li><code>seq_batch_enable</code> (bool): enable batch mode with local ring buffer.</li>
<li><code>seq_window</code> (usize): number of leaf pages to prefill into the ring per refill.</li>
<li><code>prefetch_enable</code>/<code>prefetch_window</code>: guard-mode prefetch hints to buffer pool.
Defaults are conservative; increase <code>seq_window</code> for long scans to reduce I/O hop.</li>
</ul>
</li>
</ul>
<h2 id="5-future-work"><a class="header" href="#5-future-work">5. Future Work</a></h2>
<ul>
<li>Stronger OLC with bounded retries and telemetry.</li>
<li>CSB+-like internal layout and columnar key prefixing.</li>
<li>NUMA-aware partitioning and router.</li>
<li>WAL/MVCC for crash recovery and snapshot isolation.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="storage-engine"><a class="header" href="#storage-engine">Storage Engine</a></h1>
<p>The storage engine is the foundational layer of QuillSQL, responsible for managing how data is physically stored, retrieved, and organized on disk. It acts as the bridge between the in-memory buffer pool and the on-disk database files. Its primary goal is to provide a reliable and efficient abstraction for higher-level components, like the execution engine and transaction manager, to work with data without needing to understand the intricacies of file I/O.</p>
<h2 id="core-components"><a class="header" href="#core-components">Core Components</a></h2>
<p>The storage engine is built around a few key abstractions:</p>
<ul>
<li><strong><code>DiskManager</code></strong>: The lowest-level component that directly reads and writes raw bytes from the database file in fixed-size blocks called pages.</li>
<li><strong><code>TableHeap</code></strong>: Represents the collection of pages that hold the actual data for a single table. It manages the layout of tuples within these pages.</li>
<li><strong><code>Tuple</code></strong>: The internal representation of a single row of data, containing the values for each column.</li>
</ul>
<hr />
<h2 id="tuple-storagetuplers"><a class="header" href="#tuple-storagetuplers">Tuple (<code>storage/tuple.rs</code>)</a></h2>
<p>A <code>Tuple</code> is the most basic unit of data, representing a single record or row.</p>
<h3 id="structure"><a class="header" href="#structure">Structure</a></h3>
<ul>
<li><strong><code>data: Vec&lt;ScalarValue&gt;</code></strong>: A vector where each element is a <code>ScalarValue</code>, an enum that can hold any of the database's supported data types (e.g., <code>Int32</code>, <code>Varchar</code>, <code>Bool</code>).</li>
<li><strong><code>schema: SchemaRef</code></strong>: A shared reference (<code>Arc&lt;Schema&gt;</code>) to the tuple's schema. The schema describes the name and data type of each column, allowing the database to correctly interpret the <code>data</code> vector.</li>
</ul>
<h3 id="key-functionality"><a class="header" href="#key-functionality">Key Functionality</a></h3>
<ul>
<li><strong>Creation &amp; Merging</strong>: Provides methods to create new tuples from a vector of values and to merge multiple tuples horizontally, which is essential for implementing database joins.</li>
<li><strong>Projections</strong>: Can create a new, smaller tuple containing only a subset of its original columns based on a target schema.</li>
<li><strong>Comparison</strong>: Implements <code>Ord</code> and <code>PartialOrd</code>, allowing tuples to be compared against each other. This is critical for sorting data and for evaluating filter conditions in <code>WHERE</code> clauses.</li>
</ul>
<hr />
<h2 id="disk-manager-storagedisk_managerrs"><a class="header" href="#disk-manager-storagedisk_managerrs">Disk Manager (<code>storage/disk_manager.rs</code>)</a></h2>
<p>The <code>DiskManager</code> is the exclusive interface to the physical database file on disk. It abstracts file I/O into page-oriented operations, hiding details like file handles and offsets.</p>
<h3 id="responsibilities"><a class="header" href="#responsibilities">Responsibilities</a></h3>
<ul>
<li><strong>Page I/O</strong>: Its core responsibility is to provide <code>read_page(page_id)</code> and <code>write_page(page_id, data)</code> methods. It translates a logical <code>PageId</code> into a specific byte offset in the database file.</li>
<li><strong>Page Allocation</strong>: When a new page is needed, the <code>DiskManager</code> allocates space for it in the database file by incrementing an atomic <code>next_page_id</code> counter.</li>
<li><strong>Freelist Management</strong>: To avoid letting the database file grow indefinitely, the <code>DiskManager</code> maintains a freelist of pages that have been deallocated. When a new page is requested, it first attempts to retrieve one from the freelist before allocating a new one.</li>
<li><strong>Meta Page</strong>: It manages a special <code>MetaPage</code> at the beginning of the database file (offset 0). This page stores critical database-wide metadata, such as the <code>PageId</code> of the first page in the freelist chain.</li>
</ul>
<h3 id="direct-io"><a class="header" href="#direct-io">Direct I/O</a></h3>
<p>A key feature of the <code>DiskManager</code> is its attempt to use <strong>Direct I/O</strong> (<code>O_DIRECT</code> on Linux).</p>
<ul>
<li><strong>Bypassing OS Cache</strong>: Direct I/O allows the database to bypass the operating system's page cache and write directly to the disk hardware. This prevents a "double caching" problem where data exists once in the database's own buffer pool and again in the OS cache. Giving the database exclusive control over caching is crucial for predictable performance and implementing its own buffer replacement policies (like LRU-K).</li>
<li><strong>Alignment</strong>: Direct I/O imposes strict memory alignment requirements. The <code>DiskManager</code> handles this by using a special <code>AlignedPageBuf</code> to ensure that all data buffers passed to the OS for reading or writing are aligned to the page size boundary.</li>
<li><strong>Fallback</strong>: If Direct I/O is not supported by the filesystem, the <code>DiskManager</code> gracefully falls back to standard, buffered I/O.</li>
</ul>
<hr />
<h2 id="table-heap-storagetable_heaprs"><a class="header" href="#table-heap-storagetable_heaprs">Table Heap (<code>storage/table_heap.rs</code>)</a></h2>
<p>A <code>TableHeap</code> organizes the set of pages that belong to a single table. It's called a "heap" because tuples are not stored in any specific order within the pages; they are simply appended as they come.</p>
<h3 id="structure--page-management"><a class="header" href="#structure--page-management">Structure &amp; Page Management</a></h3>
<ul>
<li>A <code>TableHeap</code> doesn't own the pages themselves but manages their <code>PageId</code>s. It keeps track of the <code>first_page_id</code> and <code>last_page_id</code> to maintain a linked list of pages for the table.</li>
<li>It interacts with the <code>BufferManager</code> to fetch pages into memory for reading or modification. It does <strong>not</strong> call the <code>DiskManager</code> directly.</li>
</ul>
<h3 id="tuple--version-management"><a class="header" href="#tuple--version-management">Tuple &amp; Version Management</a></h3>
<ul>
<li><strong>Insertion (<code>insert_tuple</code>)</strong>: When a new tuple is inserted, the <code>TableHeap</code> fetches the last page of the table. If there is enough free space, the tuple is inserted there. If the page is full, it requests a completely new page from the buffer pool, links it to the end of the page list, and inserts the tuple there.</li>
<li><strong>MVCC Support</strong>: The <code>TableHeap</code> is central to QuillSQL's Multi-Version Concurrency Control (MVCC) strategy.
<ul>
<li>An <code>UPDATE</code> operation does not modify a tuple in-place. Instead, it calls <code>mvcc_update</code>, which creates a <strong>new version</strong> of the tuple (often in a different location) and uses transaction metadata to link the old and new versions together in a version chain.</li>
<li>A <code>DELETE</code> operation simply marks a tuple version as deleted by updating its metadata; it doesn't immediately remove the data.</li>
</ul>
</li>
<li><strong>Iteration</strong>: It provides a <code>TableIterator</code> that allows the execution engine to perform a sequential scan over every tuple in the table. The iterator traverses the linked list of pages and iterates over the tuples within each page.</li>
</ul>
<h3 id="integration-with-write-ahead-logging-wal"><a class="header" href="#integration-with-write-ahead-logging-wal">Integration with Write-Ahead Logging (WAL)</a></h3>
<ul>
<li>The <code>TableHeap</code> is a key participant in ensuring data durability. Before it modifies a page (e.g., by inserting a tuple), it first generates a corresponding WAL record (e.g., <code>HeapInsertPayload</code>).</li>
<li>This record is passed to the <code>WalManager</code>, which writes it to the log. Only after the log record is secured can the actual page in the buffer pool be modified. This "log first" rule ensures that if the system crashes, the recovery process can replay the WAL to restore all committed changes.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
